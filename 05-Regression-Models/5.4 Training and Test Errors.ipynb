{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Training and Test Errors\n",
    "\n",
    "So far, we have fit regression models to data and obtained predictions from them, but we have not evaluated whether these predictions were any good. In this lesson, we will discuss different performance metrics that can be used to evaluate predictions from a machine learning model. These performance metrics can be calculated on training data or on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract the training data.\n",
    "data_dir = \"https://dlsun.github.io/pods/data/\"\n",
    "bordeaux_df = pd.read_csv(data_dir + \"bordeaux.csv\",\n",
    "                          index_col=\"year\")\n",
    "bordeaux_train = bordeaux_df.loc[:1980].copy()\n",
    "bordeaux_train[\"log(price)\"] = np.log(bordeaux_train[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics for Regression Models\n",
    "\n",
    "To evaluate the performance of a regression model, we check the predicted labels from the model against the true labels. Since the labels are quantitative, all of the performance metrics are based on the difference between each predicted label $\\hat y$ and the true label $y$. \n",
    "\n",
    "One way to make sense of these differences is to square each difference and average the squared differences. This measure of error is known as _mean squared error_ (or _MSE_, for short):\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\textrm{MSE} &= \\textrm{mean of } (y - \\hat y)^2.\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "MSE is difficult to interpret because its units are the square of the units of the label. To make MSE more interpretable, it is common to take the _square root_ of the MSE to obtain the _root mean squared error_ (or _RMSE_, for short):\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\textrm{RMSE} &= \\sqrt{\\textrm{MSE}}.\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "The RMSE measures how off a \"typical\" prediction is. Notice that this reasoning is exactly the same reasoning that we used in Chapter 3 when we defined the standard deviation as the square root of the variance.\n",
    "\n",
    "Another common measure of error is the _mean absolute error_ (or _MAE_, for short):\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\textrm{MAE} &= \\textrm{mean of } |y - \\hat y|.\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "Like the RMSE, the MAE measures how off a \"typical\" prediction is. \n",
    "\n",
    "MSE, RMSE, and MAE are all error metrics; we want them to be as small as possible. There are also performance metrics that we seek to maximize. One example is $R^2$, also known as the _coefficient of determination_:\n",
    "\n",
    "\\begin{align*}\n",
    "R^2 &= 1 - \\frac{\\text{mean of } (y - \\hat y)^2}{\\text{mean of } (y - \\bar y)^2}.\n",
    "\\end{align*}\n",
    "\n",
    "Notice that the denominator, $\\text{mean of } (y - \\bar y)^2$, is just the variance of the label $y$. So the interpretation of $\\frac{\\text{mean of } (y - \\hat y)^2}{\\text{mean of } (y - \\bar y)^2}$ is the fraction of the variance in the label $y$ that is \"left over\" after we fit the regression model. Therefore, $R^2$ can be interpreted as the fraction of variance that is explained by the regression model. It cannot be greater than $1.0$, but it can sometimes be negative if the regression model is worse than useless.\n",
    "\n",
    "These are just some of the performance metrics that are used to evaluate regression models. For more, refer to the [scikit-learn documentation on regression metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Error\n",
    "\n",
    "To calculate the performance metrics above, we need data where the true labels are known. Where do we find such data? One natural source of labeled data is the training data, since we needed the true labels to be able to train a model.\n",
    "\n",
    "For a $k$-nearest neighbors model, the training data is the data from which the $k$-nearest neighbors are selected. So to calculate the training RMSE, we do the following:\n",
    "\n",
    "For each observation in the training data:\n",
    "1. Find its $k$-nearest neighbors in the training data.\n",
    "2. Average the labels of the $k$-nearest neighbors to obtain the predicted label.\n",
    "3. Compare the predicted label to the true label.\n",
    "\n",
    "At this point, we can average the square of these differences to obtain the MSE or average their absolute values to obtain the MAE.\n",
    "\n",
    "Let's calculate the training MSE for the 5-nearest neighbors model that we fit in Chapter 5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train = bordeaux_train[[\"win\", \"summer\"]]\n",
    "y_train = bordeaux_train[\"log(price)\"]\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "          StandardScaler(),\n",
    "          KNeighborsRegressor(n_neighbors=5)\n",
    ")\n",
    "\n",
    "pipeline.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the training error, we need its predictions on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model predictions on the training data.\n",
    "y_train_ = pipeline.predict(X=X_train)\n",
    "y_train_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the predictions `y_train_` (note the trailing underscore) to the true labels `y_train`, which are known, since this is the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean-squared error.\n",
    "mse = ((y_train - y_train_) ** 2).mean()\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have also used a scikit-learn function to calculate mean-squared error. The scikit-learn functions for the performance metrics discussed in this chapter are shown in the table below. All of these functions take a 1D-array of the true labels as the first parameter and a 1D-array of the predicted labels as the second.\n",
    "\n",
    "| Metric | Function Name |\n",
    "|--------|---------------|\n",
    "| MSE | `mean_squared_error` |\n",
    "| MAE | `mean_absolute_error` |\n",
    "| $R^2$ | `r2_score` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_train, y_train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a measure of error that is more interpretable, we can take the square root to obtain the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE says that the model's predictions are off by about 0.4 on average. This is not too bad, since vintage quality ranges from 2.0 to 5.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Test Error\n",
    "\n",
    "Training error is not a great measure of the quality of a model. To see why, consider a 1-nearest neighbor regression model. Before you read on, can you guess what the training error of a 1-nearest neighbor regression model will be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a 1-nearest neighbors model.\n",
    "model = KNeighborsRegressor(n_neighbors=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the model predictions on the training data.\n",
    "y_train_ = model.predict(X_train)\n",
    "\n",
    "# Calculate the MSE\n",
    "mean_squared_error(y_train, y_train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error of this model seems too good to be true. Can our model really have an error of $0.0$? The reason, of course, is that the nearest neighbor to any observation in the training data is the observation itself!\n",
    "\n",
    "Ultimately, the goal of a machine learning model is to make predictions on *future* data. Therein lies the problem with training error. Training error measures how well a model does on the current data. It is possible to build a model that _overfits_ to the training data---that is, a model that fits so well to the current data that it does poorly on future data.\n",
    "\n",
    "For example, consider fitting two different models to the 10 training observations shown below. The model represented by the red line actually passes through every observation (that is, its training error is zero). However, most people would prefer the model represented by the blue line. If one had to make a prediction for the label when $x = 0.8$, the value of the blue line is intuitively more plausible than the value of the red line, which is out of step with the nearby points.\n",
    "\n",
    "![](overfitting.png)\n",
    "\n",
    "To make a case for the blue model using mean-squared error, we would need future, or test, data. The red model is as good as it gets when it comes to the training data.\n",
    "\n",
    "The prediction error on future data is also known as _test error_. But to calculate the test error, we need _labeled_ future data. In many applications, data is hard to collect and _labeled_ data is harder still. In the next lesson, we discuss strategies for approximating the test error using only the training data that we aleady have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Calculate the training coefficient of determination ($R^2$) of the $10$-nearest neighbors regression model that we fit in the lesson of Chapter 5.3 to the Ames housing data set (http://dlsun.github.io/pods/data/AmesHousing.txt )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Using the Tips data set (http://dlsun.github.io/pods/data/tips.csv ), train $k$-nearest neighbors regression models to predict the tip, for several different values of $k$. Calculate the training MAE of each model and make a graph showing this training error as a function of $k$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
