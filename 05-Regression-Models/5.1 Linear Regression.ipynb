{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Linear Regression\n",
    "\n",
    "In 1991, Orley Ashenfelter, an economics professor at Princeton University, stunned the wine world with a bold prediction. He predicted that the 1990 vintage of Bordeaux wines would be the \"wine of the century,'' even better than the prized 1961 vintage. Furthermore, he made this prediction without tasting even a drop of the wine, which had been placed in oak barrels just months earlier.\n",
    "\n",
    "How did Ashenfelter predict the quality of the wine without tasting it? He used data on past vintages to come up with the following formula for predicting wine quality:\n",
    "\n",
    "\\begin{align}\n",
    "    \\widehat{\\text{wine quality}} = -7.8 &+ 0.62 \\cdot (\\text{average summer temperature}) \\nonumber \\\\\n",
    "    &+ 0.0012 \\cdot (\\text{winter rainfall}) \\nonumber \\\\\n",
    "    &- 0.0037 \\cdot (\\text{harvest rainfall}) \\nonumber \\\\\n",
    "    &+ 0.024 \\cdot (\\text{age of the wine})\n",
    "    \\label{eq:ashenfelter}\n",
    "\\end{align}\n",
    "\n",
    "The variable on the left-hand side of this expression, wine quality, is what we are trying to predict and is called \n",
    "the _target_ (or _label_). (The hat symbol over  \"wine quality\" indicates that the values are predicted instead of observed.) The variables on the right-hand side, such as \"average summer temperature\" and \"harvest rainfall,\" are called _features_ and are the inputs used to predict the target. Although Ashenfelter had no way of knowing the quality of the 1990 wines, he did have the values of the features in 1990, so to make a prediction, all he had to do was plug those values into the equation above. In this way, he arrived at the following prediction for the quality of the 1990 Bordeaux, after they had been aged for 31 years (like the 1961 Bordeaux had been at the time):\n",
    "\n",
    "\\begin{align}\n",
    "    -7.8 &+ 0.62 \\cdot (18.7) \\nonumber \\\\\n",
    "    &+ 0.0012 \\cdot (468) \\nonumber \\\\\n",
    "    &- 0.0037 \\cdot (80) \\nonumber \\\\\n",
    "    &+ 0.024 \\cdot (31) = 4.8.  \\label{eq:ashenfelter_1990}\n",
    "\\end{align}\n",
    "\n",
    "For comparison, the quality of the prized 1961 vintage was 4.6.\n",
    "\n",
    "You can imagine the uproar from wine experts, who had spent years refining their palates to distinguish good wines from bad. Robert Parker, the most influential wine critic in America, called Ashenfelter's predictions \"ludicrous and absurd\", comparing him to a \"movie critic who never goes to see the movie but tells you how good it is based on the actors and the director.\" It did not help that Ashenfelter had also openly challenged Parker's rating of the 1986 Bordeaux. Parker thought they would be \"very good and sometimes exceptional.\" But according to Ashenfelter's formula, the low summer temperatures and high harvest rainfalls in 1986 doomed the vintage.\n",
    "\n",
    "Who was right? Thirty years later, Robert Parker ranks the 1986 Bordeaux well, but the 1990 Bordeaux wines are exceptional, with three of the six wines scoring a 98 on a 100-point scale.\n",
    "\n",
    "In this lesson, we will reproduce Ashenfelter's analysis, which is an example of _machine learning_. Machine learning is concerned with the general problem of how to use data to make predictions. The process of producing a model like Ashenfelter's from data is called _fitting_ a model (although the terms _training_ or _learning_ are also used), and the data that is used to fit the model is the\n",
    "_training data_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Familiar with the Data\n",
    "\n",
    "First, we read in the historical data that Ashenfelter used. The observational unit in this data set is the vintage, so we index this `DataFrame` by the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_dir = \"https://dlsun.github.io/pods/data/\"\n",
    "bordeaux_df = pd.read_csv(data_dir + \"bordeaux.csv\",\n",
    "                          index_col=\"year\")\n",
    "bordeaux_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **price** column is in 1981 dollars, normalized so that the 1961 Bordeaux has a price of 100. Price is a reasonable proxy for the quality of the wine. The **summer** column contains the average summer temperature (in degrees Celsius), while the **har** and **win** columns contain the harvest and winter rainfalls (in millimeters). The **sep** column stores the average temperature in September, which Ashenfelter did not include in his model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also take a peek at the end of this `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bordeaux_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `DataFrame` also contains data for vintages where the price is missing (including 1990, the vintage for which Ashenfelter made his prediction). In fact, prices are only available up to 1980, as it takes several years before wine quality can be estimated with much reliability), so only part of the `DataFrame` can be used for training. The rest of the data, where the features are known but the target is not, is called the _test data_. Machine learning fits a model to the training data, which is then used to predict the targets in the test data. The following code splits the `DataFrame` into the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bordeaux_train = bordeaux_df.loc[:1980].copy()\n",
    "bordeaux_test = bordeaux_df.loc[1980:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up: A Model with One Feature\n",
    "\n",
    "Before fitting a model that uses all of the features, we first consider a model that uses only the age of the wine to predict the price. That is, we fit a model of the form\n",
    "\\begin{equation}\n",
    "    \\widehat{\\text{price}} = b + c \\cdot \\text{age},\n",
    "    \\label{eq:simple_linear_regression}\n",
    "\\end{equation}\n",
    "where $b$ and $c$ are numbers that we will learn from the training data. Models of the form above are called _linear regression_ models. (The way in which this model is \"linear\" will become apparent in a moment.) This model only involves two variables, **age** and **price**, so we can visualize the data easily using a scatterplot (see Chapter 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bordeaux_train.plot.scatter(x=\"age\", y=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to fit models like the above to the training data, we use the scikit-learn package, which was used in Chapter 3 for transforming variables and calculating distances. However, its main purpose is to fit machine learning models, including linear regression. All models in scikit-learn are used in essentially the same way, following the three-step pattern:\n",
    "\n",
    "1. Declare the model.\n",
    "2. Fit the model to training data.\n",
    "3. Use the model to predict on test data.\n",
    "\n",
    "In the case of the linear regression model above, the code is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train = bordeaux_train[[\"age\"]]\n",
    "X_test = bordeaux_test[[\"age\"]]\n",
    "y_train = bordeaux_train[\"price\"]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X=X_train, y=y_train)\n",
    "model.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of `.fit()` are `X` for the features and `y` for the targets, which are assumed to be 2-D and 1-D arrays of numbers, respectively. So even when there is only one feature, as in this case, we still need to supply a 2-D array with one column---hence, the double brackets around `\"age\"` when defining `X_train` and `X_test`.\n",
    "\n",
    "By contrast, `.predict()` only has one parameter, `X` for the features. That is because its job is to predict the targets `y` for the given features. Note that the predictions will always be returned in the form of `numpy` arrays, no matter the type of the input data---so although we supplied `pandas` objects, `sklearn` still returned the predicted values as `numpy` arrays. The predictions are in the same order as the rows of `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are only two variables involved, the model above is a rare example of a machine learning model we can visualize. A general way to do this is to generate a fine grid of `X` values using `np.linspace()` and call `model.predict()` to get the predicted target at each of these values. We can then use these predictions to draw a curve which depicts the predicted value of `y` at each value of `X`. In the code below, we put the predictions in a `pandas` `Series`, indexed by the `X` values, and then call `.plot.line()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_new = pd.DataFrame()\n",
    "# create a sequence of 200 evenly spaced numbers from 10 to 41\n",
    "X_new[\"age\"] = np.linspace(10, 41, num=200)\n",
    "\n",
    "# create a Series out of the predicted values\n",
    "# (trailing underscore indicates fitted values)\n",
    "y_new_ = pd.Series(\n",
    "    model.predict(X_new), # y values in Series.plot.line()\n",
    "    index=X_new[\"age\"]    # x values in Series.plot.line()\n",
    ")\n",
    "\n",
    "# plot the data, then the model\n",
    "bordeaux_train.plot.scatter(x=\"age\", y=\"price\")\n",
    "y_new_.plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot is shown above. Notice that the curve is a straight line, which is why this model is called  _linear_ regression. In hindsight, this is obvious from the model equation: $b$ is simply the intercept and $c$ the slope of this line. All linear regression does is choose the intercept and slope to minimize the total distance between the points and the line---that is, between the observed and predicted prices. In mathematical terms, $b$ and $c$ are chosen to minimize\n",
    "\\begin{align}\n",
    "    & \\text{sum of } (\\text{price} - \\widehat{\\text{price}})^2 &= & & \\text{sum of } (\\text{price} - (b + c\\cdot \\text{age}))^2 \\\\\n",
    "    & \\text{over training data } &\\phantom{=} & & \\text{ over training data}\n",
    "\\end{align}\n",
    "over the training data. Since `sklearn` does this for us, it is not necessary to understand the details of this process to extract useful insights out of linear regression. However, the math is explained in the appendix of this lesson for those who are curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Do about Nonlinearity\n",
    "\n",
    "One question is whether the relationship between age and price is truly linear. In the graph above, it seems that the points deviate more from the line when prices are high than when they are low. To correct this, we need to spread out low prices and rein in high prices. In Chapter 3, we learned that this can be achieved by applying a log transformation to the prices. Let's add a column to the training data for the log-price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bordeaux_train[\"log(price)\"] = np.log(bordeaux_train[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fit a linear regression model to predict this new target. That is, in contrast to the previous model, we now fit the model \n",
    "\\begin{equation}\n",
    "    \\widehat{\\text{log(price)}} = b + c \\cdot \\text{age},\n",
    "    \\label{eq:simple_linear_regression_log}\n",
    "\\end{equation}\n",
    "where $b$ and $c$ are chosen to minimize \n",
    "\\begin{equation}\n",
    "    \\text{sum of } (\\text{log(price)} - \\widehat{\\text{log(price)}})^2 \\text{ over training data}\n",
    "\\end{equation}\n",
    "over the training data. The code below fits this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_price_model = LinearRegression()\n",
    "log_price_model.fit(X=bordeaux_train[[\"age\"]],\n",
    "                    y=bordeaux_train[\"log(price)\"])\n",
    "\n",
    "X_new = pd.DataFrame()\n",
    "X_new[\"age\"] = np.arange(10, 41, step=0.1)\n",
    "y_new_ = pd.Series(\n",
    "    log_price_model.predict(X_new),\n",
    "    index=X_new[\"age\"]\n",
    ")\n",
    "    \n",
    "bordeaux_train.plot.scatter(x=\"age\", y=\"log(price)\")\n",
    "y_new_.plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points are more evenly spread out when the target is log-price instead of price. For this reason, Ashenfelter chose log-price to be the measure of \"wine quality\" in his linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Ashenfelter's Model\n",
    "\n",
    "We are now ready to reproduce Ashenfelter's analysis. To do so, we will need to fit a linear regression model that predicts the log-price from the average summer temperature, winter rainfall, harvest rainfall, and the age of the wine. In other words, the model is of the form\n",
    "\\begin{align}\n",
    "    \\widehat{\\text{log(price)}} = b &+ c_1 \\cdot (\\text{average summer temperature}) \\nonumber \\\\\n",
    "    &+ c_2 \\cdot (\\text{winter rainfall}) \\nonumber \\\\\n",
    "    &+ c_3 \\cdot (\\text{harvest rainfall}) \\nonumber \\\\\n",
    "    &+ c_4 \\cdot (\\text{age of the wine}), \\label{eq:multiple_linear_regression}\n",
    "\\end{align}\n",
    "where $b, c_1, c_2, c_3, c_4$ are chosen to minimize \n",
    "\\begin{equation}\n",
    "    \\text{sum of } (\\text{log(price)} - \\widehat{\\text{log(price)}})^2 \\text{ over training data.}\n",
    "\\end{equation}\n",
    "This is still a _linear regression_ model, albeit a more complicated one.\n",
    "\n",
    "The code to fit this model is the natural extension of the code we wrote to fit the earlier models in this lesson. Instead of passing `bordeaux_train[[\"age\"]]` for `X`, we now supply a `DataFrame` containing all of the features we want to be in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashen_model = LinearRegression()\n",
    "ashen_model.fit(\n",
    "    X=bordeaux_train[[\"summer\", \"win\", \"har\", \"age\"]],\n",
    "    y=bordeaux_train[\"log(price)\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is much harder to visualize, since it involves five variables: four features, plus the target. Nevertheless, we can obtain predictions from it just as we did with the simpler models above. We just need to supply the values of all of the features in the model, in the same order as in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashen_model.predict(\n",
    "    X=bordeaux_test[[\"summer\", \"win\", \"har\", \"age\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication Corner: Interpreting the Model\n",
    "\n",
    "Even though we cannot visualize Ashenfelter's model, we can still \n",
    "interpret the model by examining the values of the _intercept_ $b$ and the _coefficients_ $c_1, c_2, c_3, c_4$.\n",
    "\n",
    "The coefficients are saved in the `.coef_` attribute, after the model has been fitted. (As above, the trailing underscore in `.coef_` reminds us that these are fitted values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashen_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These coefficients are in the same order as the columns of `X`. So $0.61871092$ is the coefficient for **summer**, $0.00119721$ the coefficient for **win**, and so on. If you compare these values with the model at the beginning of this lesson, you will see that they are exactly the coefficients that Ashenfelter obtained. \n",
    "\n",
    "A positive coefficient means that the predicted target _increases_ as that feature increases, while a negative coefficient means that it _decreases_ as that feature increases. Since **win** has a positive coefficient $(0.0012)$ and **har** has a negative coefficient $(-0.0037)$, we conclude from the model that Bordeaux wines tend to be best when winter rainfall is high and harvest rainfall is low.\n",
    "\n",
    "Another essential component of a linear regression model is the _intercept_, which is stored in the `.intercept_` attribute, separately from the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashen_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the intercept is the predicted value when all of the features are equal to $0$. However, this interpretation is often purely hypothetical, since it may be impossible for some features to be $0$. For example, to interpret the intercept of $-7.8$ in the model above, we would have to set **summer** equal to $0$. That is, we would have to imagine a summer in Bordeaux, France where the average temperature was $0^\\circ\\text{C}$ (i.e., freezing), which would be so catastrophic that the quality of red wine would be the least of our worries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Take a closer look at the predictions on the test data that we obtained from Ashenfelter's model using `ashen_model.predict()`. The prediction for the 1990 vintage is $4.04789338$, which is quite different from the $4.8$ that we calculated at the beginning of the lesson. How is this possible, when the fitted intercept and coefficients match exactly? How would you modify the code so that `ashen_model.predict()` returns a prediction of $4.8$ for the 1990 vintage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercises 2-4 ask you to fit linear regression models to the Ames housing data set (http://dlsun.github.io/pods/data/AmesHousing.txt ), which contains information about homes in Ames, Iowa._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Fit a linear regression model that predicts the price of a home (**SalePrice**) using square footage (**Gr Liv Area**) as the only feature. Since there are only two variables in this model, you can visualize the data and the fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\.  There are two ways to add the fitted linear regression model to a scatterplot. One way is to create a grid of `X` values and call `model.predict()` on those `X` values, as we did in the lesson. Another way is to extract the intercept and coefficient and draw a line with that intercept and slope directly. Try both, and verify that they give the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Fit a linear regression model that predicts the price of a home using square footage, number of bedrooms (**Bedroom AbvGr**), number of full bathrooms (**Full Bath**), and number of half bathrooms (**Half Bath**). Interpret the coefficients. Then, use your fitted model to predict the price of a home that is 1500 square feet, with 3 bedrooms, 2 full baths, and 1 half bath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercises 5-6 ask you to fit linear regression models to the tips data (http://dlsun.github.io/pods/data/tips.csv ), which contains information about tips collected by a waiter._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Suppose you want to predict how much a male diner will tip on a Sunday bill of \\$40.00. Fit a linear regression model to the tips data to answer this question. (Hint: You will need to convert categorical variables to quantitative variables. Refer to Chapter 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Fit a linear regression model, with no intercept, that predicts the tip from the total bill. That is, we want our predictions to be of the form \n",
    "$$ \\widehat{\\text{tip}} = c \\cdot (\\text{total bill}). $$\n",
    "where $c$ is some coefficient to be learned from the training data. \n",
    "\n",
    "(_Hint:_ `LinearRegression()` has a parameter, `fit_intercept=`, which is `True` by default.)\n",
    "    \n",
    "Plot the data and the fitted model. In practical terms, what assumption is being made when we fit a model with no intercept? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
