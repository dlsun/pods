{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 Estimating the Test Error\n",
    "\n",
    "In the previous lesson, we learned about _training error_, which is the error calculated on the training data. Training error is easy to calculate because the true labels are available in the training data. However, training error is not always a good measure of a model's quality, since a model that _overfits_ to the training data may have artificially low training error.\n",
    "\n",
    "Ideally, we would like to evaluate regression models based on their _test error_, which is the error calculated on the test data. The problem with test error is that it is usually impossible to calculate, since the true labels are rarely available in the test data. In this section, we discuss strategies for estimating the test error using only the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract the training data.\n",
    "data_dir = \"https://dlsun.github.io/pods/data/\"\n",
    "bordeaux_df = pd.read_csv(data_dir + \"bordeaux.csv\",\n",
    "                          index_col=\"year\")\n",
    "bordeaux_train = bordeaux_df.loc[:1980].copy()\n",
    "bordeaux_train[\"log(price)\"] = np.log(bordeaux_train[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Error\n",
    "\n",
    "To estimate the test error, we split the training data into a _training set_ and a _validation set_. First, the model is fit to just the data in the training set. Then, the model is evaluated based on its predictions on the validation set. Because the model did not train on any of the labels in the validation set, the validation set essentially plays the role of the test data, even though it was carved out of the training data.\n",
    "\n",
    "The prediction error on the validation set is known as the _validation error_. The validation error is an approximation to the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split our data into training and validation sets, we can use the `.sample()` function in `pandas`. Let's use this to split our data into two equal halves, which we will call `train` and `val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = bordeaux_train.sample(frac=.5)\n",
    "val = bordeaux_train.drop(train.index)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use these training/validation sets to approximate the test MSE of the 5-nearest neighbors model that we fit in Chapter 5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# extract features and label from training set\n",
    "X_train = train[[\"win\", \"summer\"]]\n",
    "y_train = train[\"log(price)\"]\n",
    "\n",
    "# define pipeline and fit to training set\n",
    "pipeline = make_pipeline(\n",
    "          StandardScaler(),\n",
    "          KNeighborsRegressor(n_neighbors=5)\n",
    ")\n",
    "pipeline.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make predictions on the validation set and calculate the validation RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# extract features and label from validation set\n",
    "X_val = val[[\"win\", \"summer\"]]\n",
    "y_val = val[\"log(price)\"]\n",
    "\n",
    "# get model's predictions on validation set\n",
    "y_val_ = pipeline.predict(X_val)\n",
    "\n",
    "# calculate RMSE on validation set\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test error is higher than the training error that we calculated in the previous lesson. In general, this will be true. It is harder for a model to predict for new observations it has not seen, than for observations it has seen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "The validation error above was calculated using only 50% of the training data, since we split the training data in half to create the validation set. As a result, the estimate is noisy.\n",
    "\n",
    "There is a cheap way to obtain a second opinion about how well our model will do on future data. Previously, we split our data at random into two halves, fitting the model to the first half and evaluating it on the second half. Because the model has not already seen the second half of the data, this approximates how well the model would perform on future data. \n",
    "\n",
    "But the way we split our data was arbitrary. We might as well swap the roles of the two halves, fitting the model to the _second_ half and evaluating it on the _first_ half. As long as the model is always evaluated on data that is different from the data that was used to train it, we have a valid estimate of test error. A schematic of this approach, known as _cross-validation_, is shown below.\n",
    "\n",
    "![](cross-validation.png)\n",
    "\n",
    "Because we will be doing all computations twice, just with different data, let's wrap the $k$-nearest neighbors algorithm above into a function called `get_val_error()`, that computes the validation error given training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_error(train, val):\n",
    "    \n",
    "    # extract features and label from training set.\n",
    "    X_train = train[[\"win\", \"summer\"]]\n",
    "    y_train = train[\"log(price)\"]\n",
    "    \n",
    "    # eefine pipeline and fit to training set\n",
    "    pipeline = make_pipeline(\n",
    "          StandardScaler(),\n",
    "          KNeighborsRegressor(n_neighbors=5)\n",
    "    )\n",
    "    pipeline.fit(X=X_train, y=y_train)\n",
    "    \n",
    "    # extract features and label from validation set\n",
    "    X_val = val[[\"win\", \"summer\"]]\n",
    "    y_val = val[\"log(price)\"]\n",
    "    \n",
    "    # get model's predictions on validation set\n",
    "    y_val_ = pipeline.predict(X_val)\n",
    "\n",
    "    # calculate RMSE on validation set\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_val_))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply this function to the training and validation sets from earlier, we get the same estimate of the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_val_error(train, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we reverse the roles of the training and validation sets, we get another estimate of the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_val_error(val, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two, somewhat independent estimates of the test error. It is common to average the two numbers to obtain an overall estimate of the test error, called the _cross-validation estimate of test error_. Notice that the cross-validation estimate uses each observation in the data exactly once. We make a prediction for each observation, but always using a model that was trained on data that does not include that observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation in scikit-learn\n",
    "\n",
    "As you know by now, scikit-learn provides functions that automate routine tasks of machine learning. For cross-validation, there is a function, `cross_val_score`, that takes in a model (or pipeline), the training data, and a scoring function, and carries out all aspects of cross-validation, including\n",
    "\n",
    "1. splitting the training data into training and validation sets\n",
    "2. fitting the model to each training set\n",
    "3. calculating the model's predictions on the corresponding validation set\n",
    "4. calculating the score of the predictions on each validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(pipeline, \n",
    "                         X=bordeaux_train[[\"win\", \"summer\"]],\n",
    "                         y=bordeaux_train[\"log(price)\"],\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv=2)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, notice that there are 2 scores. This is because scikit-learn calculated a score from each half of the data when that half served as the validation set.\n",
    "\n",
    "Second, observe that the scores are negative. This is because scikit-learn requires that a \"score\" be something that ought to be maximized. Since we want to minimize the mean-squared error, we want to maximize the *negative* mean-squared error. Therefore, the scores that are reported here are the negative of the MSE.\n",
    "\n",
    "To calculate the RMSE, we negate the negative sign and take a square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average of these two scores is the cross-validation estimate of test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $K$-Fold Cross Validation\n",
    "\n",
    "One problem with splitting the training data into two halves is that the model is now trained on only half the amount of data. This model will likely perform worse than the actual model, which is trained on all of the training data. So the cross-validation estimate of test error is unnecessarily pessimistic.\n",
    "\n",
    "We would like the size of the training set to be closer to the size of the original training data. We can do this by splitting the data into more than two subsamples. In general, we can split the data into $k$ subsamples, alternately training the data on $k-1$ subsamples and evaluating the model on the $1$ remaining subsample, i.e., the validation set. This produces $k$ somewhat independent estimates of the test error. This procedure is known as **$k$-fold cross validation**. (Be careful not to confuse this $k$ with the $k$ in $k$-nearest neighbors.) In hindsight, the  cross validation that we were doing previously is $2$-fold cross validation.\n",
    "\n",
    "A schematic of $4$-fold cross validation is shown below.\n",
    "\n",
    "![](k-folds.png)\n",
    "\n",
    "Implementing $k$-fold cross validation in scikit-learn is easy. We simply set the `cv=` parameter to the desired number of folds. For example, the following code carries $4$-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(pipeline, \n",
    "                         X=bordeaux_train[[\"win\", \"summer\"]],\n",
    "                         y=bordeaux_train[\"log(price)\"],\n",
    "                         scoring=\"neg_mean_squared_error\",\n",
    "                         cv=4)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $k$ scores are produced, one from each fold. These scores can be averaged to produce a single estimate of the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-scores).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Use cross-validation to approximate the *test* $R^2$ of the $10$-nearest neighbors regression model that we fit in the lesson of Chapter 5.3 to the Ames housing data set (http://dlsun.github.io/pods/data/AmesHousing.txt )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Using the Tips data set (http://dlsun.github.io/pods/data/tips.csv ), train a $k$-nearest neighbors regression model to predict the tip, for several different values of $k$. Estimate the test MAE of each model and make a graph showing this training error as a function of $k$. How does it compare to the graph of the training MAE?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "5.5 Estimating the Test Error.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlsun/pods/blob/master/05-Regression-Models/5.5%20Estimating%20the%20Test%20Error.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f90o6uh_mbKb",
        "colab_type": "text"
      },
      "source": [
        "# 5.5 Estimating the Test Error\n",
        "\n",
        "In the previous lesson, we learned about _training error_, which is the error calculated on the training data. Training error is easy to calculate because the true labels are available in the training data. However, training error is not always a good measure of a model's quality, since a model that _overfits_ to the training data may have artificially low training error.\n",
        "\n",
        "Ideally, we would like to evaluate regression models based on their _test error_, which is the error calculated on the test data. The problem with test error is that it is usually impossible to calculate, since the true labels are rarely available in the test data. In this section, we discuss strategies for estimating the test error using only the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFkROWt1mbKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Extract the training data.\n",
        "data_dir = \"https://dlsun.github.io/pods/data/\"\n",
        "bordeaux_df = pd.read_csv(data_dir + \"bordeaux.csv\",\n",
        "                          index_col=\"year\")\n",
        "bordeaux_train = bordeaux_df.loc[:1980].copy()\n",
        "bordeaux_train[\"log(price)\"] = np.log(bordeaux_train[\"price\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juNOStVUmbK9",
        "colab_type": "text"
      },
      "source": [
        "## Validation Error\n",
        "\n",
        "To estimate the test error, we split the training data into a _training set_ and a _validation set_. First, the model is fit to just the data in the training set. Then, the model is evaluated based on its predictions on the validation set. Because the model did not train on any of the labels in the validation set, the validation set essentially plays the role of the test data, even though it was carved out of the training data.\n",
        "\n",
        "The prediction error on the validation set is known as the _validation error_. The validation error is an approximation to the test error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-B5ji1gmbK_",
        "colab_type": "text"
      },
      "source": [
        "To split our data into training and validation sets, we can use the `.sample()` function in `pandas`. Let's use this to split our data into two equal halves, which we will call `train` and `val`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "739EZKHEmbLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = bordeaux_train.sample(frac=.5)\n",
        "val = bordeaux_train.drop(train.index)\n",
        "\n",
        "train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qjFQVb3mbLL",
        "colab_type": "text"
      },
      "source": [
        "Now let's use these training/validation sets to approximate the test MSE of the 5-nearest neighbors model that we fit in Chapter 5.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWUMwBBHmbLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# extract features and label from training set\n",
        "X_train = train[[\"win\", \"summer\"]]\n",
        "y_train = train[\"log(price)\"]\n",
        "\n",
        "# define pipeline and fit to training set\n",
        "pipeline = make_pipeline(\n",
        "          StandardScaler(),\n",
        "          KNeighborsRegressor(n_neighbors=5)\n",
        ")\n",
        "pipeline.fit(X=X_train, y=y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_d79s6AmbLT",
        "colab_type": "text"
      },
      "source": [
        "We make predictions on the validation set and calculate the validation RMSE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX3la99cmbLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# extract features and label from validation set\n",
        "X_val = val[[\"win\", \"summer\"]]\n",
        "y_val = val[\"log(price)\"]\n",
        "\n",
        "# get model's predictions on validation set\n",
        "y_val_ = pipeline.predict(X_val)\n",
        "\n",
        "# calculate RMSE on validation set\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_val_))\n",
        "rmse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X27KiScmbLc",
        "colab_type": "text"
      },
      "source": [
        "Notice that the test error is higher than the training error that we calculated in the previous lesson. In general, this will be true. It is harder for a model to predict for new observations it has not seen, than for observations it has seen!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D42o8RcsmbLf",
        "colab_type": "text"
      },
      "source": [
        "## Cross Validation\n",
        "\n",
        "The validation error above was calculated using only 50% of the training data, since we split the training data in half to create the validation set. As a result, the estimate is noisy.\n",
        "\n",
        "There is a cheap way to obtain a second opinion about how well our model will do on future data. Previously, we split our data at random into two halves, fitting the model to the first half and evaluating it on the second half. Because the model has not already seen the second half of the data, this approximates how well the model would perform on future data. \n",
        "\n",
        "But the way we split our data was arbitrary. We might as well swap the roles of the two halves, fitting the model to the _second_ half and evaluating it on the _first_ half. As long as the model is always evaluated on data that is different from the data that was used to train it, we have a valid estimate of test error. A schematic of this approach, known as _cross-validation_, is shown below.\n",
        "\n",
        "![](https://github.com/dlsun/pods/blob/master/05-Regression-Models/cross-validation.png?raw=1)\n",
        "\n",
        "Because we will be doing all computations twice, just with different data, let's wrap the $k$-nearest neighbors algorithm above into a function called `get_val_error()`, that computes the validation error given training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01HBtDu_mbLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_val_error(train, val):\n",
        "    \n",
        "    # extract features and label from training set.\n",
        "    X_train = train[[\"win\", \"summer\"]]\n",
        "    y_train = train[\"log(price)\"]\n",
        "    \n",
        "    # eefine pipeline and fit to training set\n",
        "    pipeline = make_pipeline(\n",
        "          StandardScaler(),\n",
        "          KNeighborsRegressor(n_neighbors=5)\n",
        "    )\n",
        "    pipeline.fit(X=X_train, y=y_train)\n",
        "    \n",
        "    # extract features and label from validation set\n",
        "    X_val = val[[\"win\", \"summer\"]]\n",
        "    y_val = val[\"log(price)\"]\n",
        "    \n",
        "    # get model's predictions on validation set\n",
        "    y_val_ = pipeline.predict(X_val)\n",
        "\n",
        "    # calculate RMSE on validation set\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_val_))\n",
        "    \n",
        "    return rmse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSdZrInwmbLm",
        "colab_type": "text"
      },
      "source": [
        "If we apply this function to the training and validation sets from earlier, we get the same estimate of the test error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0czXd60BmbLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_val_error(train, val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjuCPpPImbLz",
        "colab_type": "text"
      },
      "source": [
        "But if we reverse the roles of the training and validation sets, we get another estimate of the test error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYkhoxhYmbL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_val_error(val, train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4h4-IG7mbL7",
        "colab_type": "text"
      },
      "source": [
        "Now we have two, somewhat independent estimates of the test error. It is common to average the two numbers to obtain an overall estimate of the test error, called the _cross-validation estimate of test error_. Notice that the cross-validation estimate uses each observation in the data exactly once. We make a prediction for each observation, but always using a model that was trained on data that does not include that observation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF1KaaBMmbME",
        "colab_type": "text"
      },
      "source": [
        "## Cross-Validation in scikit-learn\n",
        "\n",
        "As you know by now, scikit-learn provides functions that automate routine tasks of machine learning. For cross-validation, there is a function, `cross_val_score`, that takes in a model (or pipeline), the training data, and a scoring function, and carries out all aspects of cross-validation, including\n",
        "\n",
        "1. splitting the training data into training and validation sets\n",
        "2. fitting the model to each training set\n",
        "3. calculating the model's predictions on the corresponding validation set\n",
        "4. calculating the score of the predictions on each validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFE4-YjOmbMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(pipeline, \n",
        "                         X=bordeaux_train[[\"win\", \"summer\"]],\n",
        "                         y=bordeaux_train[\"log(price)\"],\n",
        "                         scoring=\"neg_mean_squared_error\",\n",
        "                         cv=2)\n",
        "scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq1ruqX5mbMP",
        "colab_type": "text"
      },
      "source": [
        "First, notice that there are 2 scores. This is because scikit-learn calculated a score from each half of the data when that half served as the validation set.\n",
        "\n",
        "Second, observe that the scores are negative. This is because scikit-learn requires that a \"score\" be something that ought to be maximized. Since we want to minimize the mean-squared error, we want to maximize the *negative* mean-squared error. Therefore, the scores that are reported here are the negative of the MSE.\n",
        "\n",
        "To calculate the RMSE, we negate the negative sign and take a square root."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so1nP6CTmbMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.sqrt(-scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK-kEgaombMW",
        "colab_type": "text"
      },
      "source": [
        "The average of these two scores is the cross-validation estimate of test error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj-QBHBXmbMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.sqrt(-scores).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XP8WQXAmbMd",
        "colab_type": "text"
      },
      "source": [
        "## $K$-Fold Cross Validation\n",
        "\n",
        "One problem with splitting the training data into two halves is that the model is now trained on only half the amount of data. This model will likely perform worse than the actual model, which is trained on all of the training data. So the cross-validation estimate of test error is unnecessarily pessimistic.\n",
        "\n",
        "We would like the size of the training set to be closer to the size of the original training data. We can do this by splitting the data into more than two subsamples. In general, we can split the data into $k$ subsamples, alternately training the data on $k-1$ subsamples and evaluating the model on the $1$ remaining subsample, i.e., the validation set. This produces $k$ somewhat independent estimates of the test error. This procedure is known as **$k$-fold cross validation**. (Be careful not to confuse this $k$ with the $k$ in $k$-nearest neighbors.) In hindsight, the  cross validation that we were doing previously is $2$-fold cross validation.\n",
        "\n",
        "A schematic of $4$-fold cross validation is shown below.\n",
        "\n",
        "![](https://github.com/dlsun/pods/blob/master/05-Regression-Models/k-folds.png?raw=1)\n",
        "\n",
        "Implementing $k$-fold cross validation in scikit-learn is easy. We simply set the `cv=` parameter to the desired number of folds. For example, the following code carries $4$-fold cross validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYzPOCqUmbMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = cross_val_score(pipeline, \n",
        "                         X=bordeaux_train[[\"win\", \"summer\"]],\n",
        "                         y=bordeaux_train[\"log(price)\"],\n",
        "                         scoring=\"neg_mean_squared_error\",\n",
        "                         cv=4)\n",
        "scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFdffjNqmbMm",
        "colab_type": "text"
      },
      "source": [
        "Notice that $k$ scores are produced, one from each fold. These scores can be averaged to produce a single estimate of the test error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlKS2MyEmbMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.sqrt(-scores).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOnHne75mbMu",
        "colab_type": "text"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJteZGgbmbMw",
        "colab_type": "text"
      },
      "source": [
        "1\\. Use cross-validation to approximate the *test* $R^2$ of the $10$-nearest neighbors regression model that we fit in the lesson of Chapter 5.3 to the Ames housing data set (http://dlsun.github.io/pods/data/AmesHousing.txt )."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W05JdaTmbMy",
        "colab_type": "text"
      },
      "source": [
        "2\\. Using the Tips data set (http://dlsun.github.io/pods/data/tips.csv ), train a $k$-nearest neighbors regression model to predict the tip, for several different values of $k$. Estimate the test MAE of each model and make a graph showing this training error as a function of $k$. How does it compare to the graph of the training MAE?"
      ]
    }
  ]
}
>>>>>>> 3bc9d5c89ec427780cbaff9c48c95bd67e1b74ba
