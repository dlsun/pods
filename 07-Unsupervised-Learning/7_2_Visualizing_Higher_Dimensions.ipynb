{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7.2 Visualizing Higher Dimensions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTPLBCWKzfWS",
        "colab_type": "text"
      },
      "source": [
        "# 7.2 Visualizing Higher Dimensions\n",
        "\n",
        "By this point, we hope we've convinced you how important it is to visualize your data. While summary statistics are helpful, it doesn't provide us with a good grasp of what the entire dataset looks like. In two dimension, we can use a 2-D scatter plot. In three dimension, we can use a 3-D scatter plot. But what if we have more than three dimensions? This chapter talks about how we can visualize data that is beyond 3 dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1mm_fCWm9dO",
        "colab_type": "text"
      },
      "source": [
        "## Goal of Visualizing Higher Dimensions\n",
        "\n",
        ">\"I am a Tralfamadorian, seeing all time as you might see a stretch of the Rocky Mountains. All time is all time. It does not change. It does not lend itself to warnings or explanations. It simply is.\" \n",
        ">\n",
        ">-Kurt Vonnegate in \"Slaughterhour-Five\"\n",
        "\n",
        "We unfortunately are not Tralfamadorians, instead we are three dimensional beings who can't visually see a fourth dimension like its a location on the Rocky Mountains. However, this doesn't mean that the fourth dimension is meaingless to us. We can derive a lot of understand from understanding the higher dimensions. The problem is, we can't it and thus we can't plot it. \n",
        "\n",
        "Fortunately, very clever mathematicians throughout history has invented techniques to allow us to simulate what the higher dimension would look like. The rest of the section will discuss these techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR-JkcQuwozv",
        "colab_type": "text"
      },
      "source": [
        "## Using Size and Color\n",
        "\n",
        "This is more of a review from previous sections but one way to visualize more dimensions is by using the size and color attributes of your scatter plots. This is rather intuitive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgChDoxMwyKa",
        "colab_type": "code",
        "outputId": "cc578404-1e67-476a-f736-e070bc9c233e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import altair as alt\n",
        "\n",
        "df_bordeaux = pd.read_csv(\"http://dlsun.github.io/pods/data/bordeaux.csv\")\n",
        "\n",
        "alt.Chart(df_bordeaux).mark_circle().encode(\n",
        "    alt.X('age',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ),\n",
        "    alt.Y('sep',\n",
        "        scale=alt.Scale(zero=False)\n",
        "    ),\n",
        "    color=\"summer\",\n",
        "    size=\"win\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-75d16bce5abb4012b9011722a14c7363\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-75d16bce5abb4012b9011722a14c7363\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-75d16bce5abb4012b9011722a14c7363\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-48d12def23bfff0b8e5b8e7fd22dc9b6\"}, \"mark\": \"circle\", \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"summer\"}, \"size\": {\"type\": \"quantitative\", \"field\": \"win\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"age\", \"scale\": {\"zero\": false}}, \"y\": {\"type\": \"quantitative\", \"field\": \"sep\", \"scale\": {\"zero\": false}}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-48d12def23bfff0b8e5b8e7fd22dc9b6\": [{\"year\": 1952, \"price\": 37.0, \"summer\": 17.1, \"har\": 160, \"sep\": 14.3, \"win\": 600, \"age\": 40}, {\"year\": 1953, \"price\": 63.0, \"summer\": 16.7, \"har\": 80, \"sep\": 17.3, \"win\": 690, \"age\": 39}, {\"year\": 1955, \"price\": 45.0, \"summer\": 17.1, \"har\": 130, \"sep\": 16.8, \"win\": 502, \"age\": 37}, {\"year\": 1957, \"price\": 22.0, \"summer\": 16.1, \"har\": 110, \"sep\": 16.2, \"win\": 420, \"age\": 35}, {\"year\": 1958, \"price\": 18.0, \"summer\": 16.4, \"har\": 187, \"sep\": 19.1, \"win\": 582, \"age\": 34}, {\"year\": 1959, \"price\": 66.0, \"summer\": 17.5, \"har\": 187, \"sep\": 18.7, \"win\": 485, \"age\": 33}, {\"year\": 1960, \"price\": 14.0, \"summer\": 16.4, \"har\": 290, \"sep\": 15.8, \"win\": 763, \"age\": 32}, {\"year\": 1961, \"price\": 100.0, \"summer\": 17.3, \"har\": 38, \"sep\": 20.4, \"win\": 830, \"age\": 31}, {\"year\": 1962, \"price\": 33.0, \"summer\": 16.3, \"har\": 52, \"sep\": 17.2, \"win\": 697, \"age\": 30}, {\"year\": 1963, \"price\": 17.0, \"summer\": 15.7, \"har\": 155, \"sep\": 16.2, \"win\": 608, \"age\": 29}, {\"year\": 1964, \"price\": 31.0, \"summer\": 17.3, \"har\": 96, \"sep\": 18.8, \"win\": 402, \"age\": 28}, {\"year\": 1965, \"price\": 11.0, \"summer\": 15.4, \"har\": 267, \"sep\": 14.8, \"win\": 602, \"age\": 27}, {\"year\": 1966, \"price\": 47.0, \"summer\": 16.5, \"har\": 86, \"sep\": 18.4, \"win\": 819, \"age\": 26}, {\"year\": 1967, \"price\": 19.0, \"summer\": 16.2, \"har\": 118, \"sep\": 16.5, \"win\": 714, \"age\": 25}, {\"year\": 1968, \"price\": 11.0, \"summer\": 16.2, \"har\": 292, \"sep\": 16.4, \"win\": 610, \"age\": 24}, {\"year\": 1969, \"price\": 12.0, \"summer\": 16.5, \"har\": 244, \"sep\": 16.6, \"win\": 575, \"age\": 23}, {\"year\": 1970, \"price\": 40.0, \"summer\": 16.7, \"har\": 89, \"sep\": 18.0, \"win\": 622, \"age\": 22}, {\"year\": 1971, \"price\": 27.0, \"summer\": 16.8, \"har\": 112, \"sep\": 16.9, \"win\": 551, \"age\": 21}, {\"year\": 1972, \"price\": 10.0, \"summer\": 15.0, \"har\": 158, \"sep\": 14.6, \"win\": 536, \"age\": 20}, {\"year\": 1973, \"price\": 16.0, \"summer\": 17.1, \"har\": 123, \"sep\": 17.9, \"win\": 376, \"age\": 19}, {\"year\": 1974, \"price\": 11.0, \"summer\": 16.3, \"har\": 184, \"sep\": 16.2, \"win\": 574, \"age\": 18}, {\"year\": 1975, \"price\": 30.0, \"summer\": 16.9, \"har\": 171, \"sep\": 17.2, \"win\": 572, \"age\": 17}, {\"year\": 1976, \"price\": 25.0, \"summer\": 17.6, \"har\": 247, \"sep\": 16.1, \"win\": 418, \"age\": 16}, {\"year\": 1977, \"price\": 11.0, \"summer\": 15.6, \"har\": 87, \"sep\": 16.8, \"win\": 821, \"age\": 15}, {\"year\": 1978, \"price\": 27.0, \"summer\": 15.8, \"har\": 51, \"sep\": 17.4, \"win\": 763, \"age\": 14}, {\"year\": 1979, \"price\": 21.0, \"summer\": 16.2, \"har\": 122, \"sep\": 17.3, \"win\": 717, \"age\": 13}, {\"year\": 1980, \"price\": 14.0, \"summer\": 16.0, \"har\": 74, \"sep\": 18.4, \"win\": 578, \"age\": 12}, {\"year\": 1981, \"price\": null, \"summer\": 17.0, \"har\": 111, \"sep\": 18.0, \"win\": 535, \"age\": 11}, {\"year\": 1982, \"price\": null, \"summer\": 17.4, \"har\": 162, \"sep\": 18.5, \"win\": 712, \"age\": 10}, {\"year\": 1983, \"price\": null, \"summer\": 17.4, \"har\": 119, \"sep\": 17.9, \"win\": 845, \"age\": 9}, {\"year\": 1984, \"price\": null, \"summer\": 16.5, \"har\": 119, \"sep\": 16.0, \"win\": 591, \"age\": 8}, {\"year\": 1985, \"price\": null, \"summer\": 16.8, \"har\": 38, \"sep\": 18.9, \"win\": 744, \"age\": 7}, {\"year\": 1986, \"price\": null, \"summer\": 16.3, \"har\": 171, \"sep\": 17.5, \"win\": 563, \"age\": 6}, {\"year\": 1987, \"price\": null, \"summer\": 17.0, \"har\": 115, \"sep\": 18.9, \"win\": 452, \"age\": 5}, {\"year\": 1988, \"price\": null, \"summer\": 17.1, \"har\": 59, \"sep\": 16.8, \"win\": 808, \"age\": 4}, {\"year\": 1989, \"price\": null, \"summer\": 18.6, \"har\": 82, \"sep\": 18.4, \"win\": 443, \"age\": 3}, {\"year\": 1990, \"price\": null, \"summer\": 18.7, \"har\": 80, \"sep\": 19.3, \"win\": 468, \"age\": 2}, {\"year\": 1991, \"price\": null, \"summer\": 17.7, \"har\": 183, \"sep\": 20.4, \"win\": 570, \"age\": 1}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxV83MG_y2LJ",
        "colab_type": "text"
      },
      "source": [
        "I am sure you can see the limitations of this method: you can only go up for 4 dimensions (5 if you use a 3-D scatter plot). This is still worth mentioning as sometimes, this may be all you need. \n",
        "\n",
        "For higher dimensions, we should consider either feature selection or feature reduction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlWDTyDsr5Zv",
        "colab_type": "text"
      },
      "source": [
        "## Feature Selection \n",
        "\n",
        "If we want to compress 10 dimensions worth of data into 2 dimensions, we're bound to lose some detail during that compression. We can measure how much detail we kept at the end with the explained variance ratio which gives the percentage of variance/detail we kept after the compression. The higher the ratio, the more variance and detail we kept. \n",
        "\n",
        "One way very simple, almost trivial, way to only visualize higher dimensional data is to only plot the two dimensions that explains the most variation in the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9kxLPD30dxa",
        "colab_type": "code",
        "outputId": "c1aedb62-573b-406c-cfa6-844ab4054122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "sclr = StandardScaler()\n",
        "df_bordeaux = pd.DataFrame(sclr.fit_transform(df_bordeaux.dropna()), columns=df_bordeaux.columns)\n",
        "\n",
        "X = df_bordeaux.drop(\"price\", axis=1)\n",
        "y = df_bordeaux[\"price\"]\n",
        "\n",
        "reg = LinearRegression()\n",
        "reg.fit(X, y)\n",
        "\n",
        "scores = pd.Series(dtype=float, name=\"R^2 Values\")\n",
        "for column in X.columns: \n",
        "    reg = LinearRegression()\n",
        "    reg.fit(X[[column]], y)\n",
        "    scores[column] = reg.score(X[[column]], y)\n",
        "\n",
        "scores.sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "summer    0.343538\n",
              "sep       0.323582\n",
              "age       0.206936\n",
              "year      0.206936\n",
              "har       0.199621\n",
              "win       0.053456\n",
              "Name: R^2 Values, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHprVgDLxKyE",
        "colab_type": "text"
      },
      "source": [
        "As we can see above, average summer temperature **summer** and average september temperature **sep** are the two variables that explain the most variance in the quality of the wine **price**. Thus, if we want to get the best representation of the dataset with only two dimensions, we can make a scatterplot of **summer** vs **sep**. However, even with the two variables that explain the most variation, we can only capture 33% of the variation of the original data. The other 66% is lost to the other features we chose to ignore. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcUCeYGOx2nP",
        "colab_type": "code",
        "outputId": "16dc19dd-f2ac-44b7-8ac6-0236cac283d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "explained_var = X[[\"summer\", \"sep\"]].var(axis=0).sum() / X.var(axis=0).sum() \n",
        "\n",
        "print(\"% Variance Explained:\", explained_var, end=\"\\n\\n\")\n",
        "df_bordeaux.plot.scatter(x=\"summer\", y=\"sep\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% Variance Explained: 0.3333333333333333\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7e1fdaecc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6gWdO1CINp-",
        "colab_type": "text"
      },
      "source": [
        "Additionally, if we look below, using only two features has hindered our predictive accuracy. This sucks! Fortunately, some very clever mathematicians came up with ways to get around this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7re2E6KHUbs",
        "colab_type": "code",
        "outputId": "24c2f5c2-ff71-420e-874c-2bbdee6f9a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "reg = LinearRegression()\n",
        "\n",
        "reg.fit(X, y)\n",
        "print(\"All Features:\\t\\t\", reg.score(X, y))\n",
        "reg.fit(X[[\"summer\", \"sep\"]], y)\n",
        "print(\"With PCA Features:\\t\", reg.score(X[[\"summer\", \"sep\"]], y), end=\"\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Features:\t\t 0.7526018827767169\n",
            "With PCA Features:\t 0.4633153344681292\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tflTZWS688xW",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality Reduction\n",
        "\n",
        "With feature selection, we were only able to capture 33% of the original variance, which isn't great. To capture more variation while still remaining in two variables, have to utilize some clever math.\n",
        "\n",
        "These clever mathematical techniques are known as feature creation, where we try to create new variables that helps us visualize higher dimensional data. There are many different techniques for dimensionality reduction all of which attempts to accomplish different things. Let's start off with the simplest and most popular one: **Principle Component Analysis**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ52OQlGAm6Q",
        "colab_type": "text"
      },
      "source": [
        "### Principle Component Analysis (PCA)\n",
        "\n",
        "Simply put, Principle Component Analysis create new features that maximizes variation. What PCA is **not** doing is feature selection, rather it is creating an entirely new arbitrary feature that is a combination of all the features. \n",
        "\n",
        "PCA involves some simple linear algebra, but SciKit-Learn has a PCA implementation. Note that all dimensionality reduction algorithms in SciKit-Learn is operated very similarity to machine learning algorithms you learned in the previous chapters. Create the object and then run `fit()` or `fit_transform()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FESc4NoI8-3v",
        "colab_type": "code",
        "outputId": "1d1b00d1-65f5-40dd-ca5f-93f9ba84ef7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# We want to be able to plot this on a 2-D scatter plot so we choose 2 Dimensions\n",
        "dimension_we_want = 2\n",
        "\n",
        "pca = PCA(n_components=dimension_we_want)\n",
        "X_2d = pca.fit_transform(X) \n",
        "X_2d = pd.DataFrame(X_2d, columns=[\"PCA1\", \"PCA2\"])\n",
        "\n",
        "print(\"\\n% Variance Explained:\", pca.explained_variance_ratio_.sum(), end=\"\\n\\n\")\n",
        "X_2d.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "% Variance Explained: 0.6462543462926849\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PCA1</th>\n",
              "      <th>PCA2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.620822</td>\n",
              "      <td>-1.437859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.166446</td>\n",
              "      <td>0.732196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.359265</td>\n",
              "      <td>-0.067934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.518218</td>\n",
              "      <td>-0.792417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.519920</td>\n",
              "      <td>0.451721</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PCA1      PCA2\n",
              "0  2.620822 -1.437859\n",
              "1  2.166446  0.732196\n",
              "2  2.359265 -0.067934\n",
              "3  1.518218 -0.792417\n",
              "4  1.519920  0.451721"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj87JXmmBdNu",
        "colab_type": "code",
        "outputId": "4574186e-9f94-4b91-9cf9-ba4a68b9f866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "X_2d.plot.scatter(x=\"PCA1\", y=\"PCA2\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7e1cb29278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT7Rv9lEDSji",
        "colab_type": "text"
      },
      "source": [
        "As we can see from above, the two new features (known as Principle Components) are not like any of our input features. Additionally, these two new components explains 64.6% of all the original variation. Let's see how well this performs in explaining our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBHVGhd-CJF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg = LinearRegression()\n",
        "\n",
        "reg.fit(X, y)\n",
        "print(\"All Features:\\t\\t\", reg.score(X, y))\n",
        "reg.fit(X_2d, y)\n",
        "print(\"With PCA Features:\\t\", reg.score(X_2d, y), end=\"\\n\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsoN_a8XI4p2",
        "colab_type": "text"
      },
      "source": [
        "As expected, with a higher explained variance ratio, we perform better in predicting the quality of the wine. With Dimensionality Reduction, we were able to capture most of the variation in the dataset while still being able to view it in two dimensions. In the most basic of terms, PCA creates a variable projected along the axis of maximum variable.\n",
        "\n",
        "![](https://raw.githubusercontent.com/bfkwong/data/master/IMG_0187%202.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwDiZRX3JYLN",
        "colab_type": "text"
      },
      "source": [
        "### Optional: Linear Algebra Behind PCA\n",
        "\n",
        "Principle Component Analysis chooses principle components along te **axis of greatest variance**. In Linear Algebra terms, the axis of greatest variance is the **Eigenvector with the largest Eigenvalue of the covariance matrix ($\\Sigma$)**\n",
        "\n",
        "The following are the steps in order to do PCA manually. \n",
        "\n",
        "1. Given data matrix $M$, generate the covariance matrix of $M$ denoted as $\\Sigma$\n",
        "2. We then compute the Eigenvector and Eigenvalue of covariance matrix $\\Sigma$\n",
        "3. Project the features to the Eigenvector with the largest Eigenvalue using the dot product (cross product for more than 1 dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J26dEAoAD0q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Calculate covariance matrix \n",
        "cov_mtrx = np.cov(X.T)\n",
        "\n",
        "# Step 2: Calculate Eigenvector and Eigenvalue\n",
        "W,v = np.linalg.eig(cov_mtrx)\n",
        "\n",
        "# Step 3: Find the largest Eigenvalue and project our data onto the corresponding Eigenvector\n",
        "idx_largest_eigenval = np.argmax(W)\n",
        "eigenvec = v[:,idx_largest_eigenval]\n",
        "\n",
        "total = []\n",
        "for row in X.index: \n",
        "    total.append(np.dot(X.loc[row], eigenvec))\n",
        "\n",
        "pd.DataFrame(pd.Series(total), columns=[\"PCA1\"]).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G7NWh_1nf_k",
        "colab_type": "text"
      },
      "source": [
        "One interesting thing you may see here is that the eigenvalue corresponds to the variance explained by its corresponding eigenvalue. The eigenvalue of PCA1 is 2.26 and the variance of PCA1 is also 2.26"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QCALn43iTYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_largest_eigenval = np.argmax(W)\n",
        "variance = pd.Series(total).var()\n",
        "\n",
        "print(\"Largest Eigenvalue:\\t\", W[idx_largest_eigenval])\n",
        "print(\"Variance of Eigenvector:\", variance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmLOaYzvEeG7",
        "colab_type": "text"
      },
      "source": [
        "### Multidimensional Scaling (MDS)\n",
        "\n",
        "We now pay a visit to our good friend, the Euclidean distance. One incredibly useful aspect of Euclidean distance is that it works in higher dimensions. The formula $$\\sqrt{x_1^2 + x_2^2}$$ is for two dimensional Euclidean distance, but to move it to a third dimension, it is as easy as adding a $x^3$ variable. One thing to recognize is that Euclidean distance in all dimension is still a number. \n",
        "\n",
        "Why am I rambling on about something you learned in middle school? Well the realization that Euclidean distance is scalar in all dimensions means that we can preserve the variance of n-th dimensional data in two dimensions as long as we try to ensure that the Euclidean distances in the n-th dimensional is proportion to the Euclidean distance in 2 dimensions. That was a lot to take in, the following image explains the concept.\n",
        "\n",
        "![](https://raw.githubusercontent.com/bfkwong/data/master/IMG_0188.jpg)\n",
        "\n",
        "Notice how when we reduced our dimensions from 2 to 1, the distances between points A, B, and C remained the same. Meaning that x, y, and z remained the same between the two dimensions. While distances may not always be preserved perfectly between dimensions, MDS attempts to preserve it as well as possible.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4AjVFrwn78I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import MDS\n",
        "# We want to be able to plot this on a 2-D scatter plot so we choose 2 Dimensions\n",
        "dimension_we_want = 2\n",
        "\n",
        "mds = MDS(n_components=dimension_we_want)\n",
        "X_2d = mds.fit_transform(X) \n",
        "X_2d = pd.DataFrame(X_2d, columns=[\"Dimension 1\", \"Dimension 2\"])\n",
        "\n",
        "display(X_2d.head())\n",
        "print(\"Percentage variance explained:\", X_2d.var().sum()/X.var().sum())\n",
        "X_2d.plot.scatter(x=\"Dimension 1\", y=\"Dimension 2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlIA-o_cEnSD",
        "colab_type": "text"
      },
      "source": [
        "By using MDS, we were actually able to preserve over 95% of the variance from the original datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88kI9JxzFO6n",
        "colab_type": "text"
      },
      "source": [
        "### Linear vs Nonlinear Dimensionality Reduction \n",
        "\n",
        "Thus far, we have explored PCA (a linear reduction technique) and MDS (a nonlinear reduction technique). While there are a lot of differences between the two methods. The key differences could be boiled down to just the following statement: **linear dimensionality reduction technique only stretch and shift the data while nonlinear techniques make more drastic changes to the data**.\n",
        "\n",
        "This sometimes leads to nonlinear techniques being better at capturing variance but losing the overall shape of the data whereas linear techniques are better at keeping the general shape of the original data but loses more variance along the way. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uprQWlUGC37",
        "colab_type": "text"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnMJKgsTkDoZ",
        "colab_type": "text"
      },
      "source": [
        "1. Consider the Iris dataset (https://raw.githubusercontent.com/dlsun/pods/master/data/iris.csv). Drop the \"SepalWidth\" and \"PedalWidth\" columns and then apply PCA on \"SepalLength\" and \"PedalLength\" with `n_components = 2`. How many percent of the variance was PCA able to capture in this case? What happens when we use PCA to compress 2D data into 2D data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCXKtA8AlSQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}